{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import generation_types\n",
    "import google.generativeai as genai #pip install google-generativeai\n",
    "\n",
    "import zlib\n",
    "import json\n",
    "import time\n",
    "from base64 import b64decode, b64encode\n",
    "\n",
    "API_KEY = \"AIzaSyB8-HF9UG7xnCpUcVutdCuep0tiR_GvuZg\" \n",
    "genai.configure(api_key=API_KEY)\n",
    "    \n",
    "def get_users(filepath:str) -> dict: \n",
    "    with open(filepath, \"r\") as f: \n",
    "        users = f.read()\n",
    "        f.close()\n",
    "    return json.loads(users)\n",
    "\n",
    "\n",
    "def parse_geminichat_history(gemini_chat_history): \n",
    "    gemini_chat_history_items = []\n",
    "    for content in gemini_chat_history: \n",
    "        gemini_chat_history_items.append({\n",
    "            \"role\": content.role,\n",
    "            \"parts\": [\n",
    "                { \"text\": part.text } for part in content.parts\n",
    "            ]\n",
    "        })\n",
    "    return gemini_chat_history_items\n",
    "\n",
    "def get_history(user, chat_title:str = \"\") -> dict: \n",
    "    b64_history = bytes(user[\"history\"], \"utf-8\")\n",
    "    decompressed_history = zlib.decompress(\n",
    "        b64decode(b64_history)\n",
    "    )\n",
    "    history = json.loads(decompressed_history.decode())\n",
    "    \n",
    "    if chat_title: \n",
    "        return history.get(chat_title, {})\n",
    "\n",
    "    return history\n",
    "\n",
    "def set_history(user, gemini_chat_history, chat_title:str) -> None: \n",
    "    history = get_history(user)\n",
    "    history[chat_title] = parse_geminichat_history(gemini_chat_history)\n",
    "    # print(json.dumps(history))\n",
    "    # print()\n",
    "\n",
    "    bytes_history = bytes(json.dumps(history), \"utf-8\")\n",
    "    compressed_history = zlib.compress(bytes_history)\n",
    "    b64_history = b64encode(compressed_history).decode()\n",
    "    user[\"history\"] = b64_history\n",
    "\n",
    "def get_ai_response(user, prompt=\"hi\"): \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    selected_chat = user.get(\"selected_chat\", \"\")\n",
    "\n",
    "    if selected_chat: \n",
    "        history = get_history(user, selected_chat)\n",
    "        chat = model.start_chat(history=history)\n",
    "        response = chat.send_message(prompt)\n",
    "        set_history(user, chat.history, selected_chat)\n",
    "    else: \n",
    "        chat = model.start_chat()\n",
    "        response = chat.send_message(prompt)\n",
    "        judul = chat.send_message(\"berikan satu judul yang cocok untuk percakapan ini, judulnya saja!\")\n",
    "        user[\"selected_chat\"] = judul.text\n",
    "        set_history(user, chat.history, judul.text)\n",
    "\n",
    "    return response\n",
    "\n",
    " \n",
    "# users = {\n",
    "#     \"6281818475959\":{\n",
    "#         \"history\" : \"eJyrrgUAAXUA+Q==\",\n",
    "#         \"selected_chat\" : None\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, List, Dict, Any, Optional, Type\n",
    "import textwrap\n",
    "\n",
    "def pretty_class(\n",
    "    obj: Any, \n",
    "    indent_width: int = 2, \n",
    "    iter: int = 1\n",
    ") -> str: \n",
    "    \"\"\"\n",
    "    Return a string representation of the given object with a pretty-printed format.\n",
    "\n",
    "    Parameters:\n",
    "        obj (Any): The object to be pretty-printed.\n",
    "        indent_width (int): The number of spaces for each level of indentation (default is 2).\n",
    "        iter (int): The current iteration level used for indentation (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the object in a pretty-printed format.\n",
    "    \"\"\"\n",
    "    obj_name = obj.__class__.__name__\n",
    "    result = f\"{obj_name}(\"\n",
    "\n",
    "    for key, value in obj.__dict__.items():\n",
    "        indent = \" \" * (indent_width * iter)\n",
    "\n",
    "        if hasattr(value, \"__dict__\"):\n",
    "            result += f\"\\n{indent}{key}=\" + pretty_class(value, indent_width, iter + 1) + \",\"\n",
    "        else: \n",
    "            value = f\"\\\"{value}\\\"\" if isinstance(value, str) else value\n",
    "            result += f\"\\n{indent}{key}={value},\"\n",
    "    \n",
    "    # remove extra comma\n",
    "    result = result[:-1]\n",
    "\n",
    "    outer_indent = \" \" * (indent_width * (iter - 1))\n",
    "    result += f\"\\n{outer_indent})\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "class AiChatHistory: \n",
    "    \"\"\"\n",
    "    Represents a chat history for AI-generated conversations.\n",
    "\n",
    "    Attributes:\n",
    "        data (str): The chat history data.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data:str): \n",
    "        \"\"\"\n",
    "        Initializes a new instance of AiChatHistory.\n",
    "\n",
    "        Parameters:\n",
    "            data (str): The chat history data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def get_history(self, chat_title: Optional[str] = \"\") -> Dict[str, Any]: \n",
    "        \"\"\"\n",
    "        Retrieve the chat history data, optionally filtered by chat title.\n",
    "\n",
    "        Parameters:\n",
    "            chat_title (str, optional): The title of the chat to filter the history (default is \"\").\n",
    "\n",
    "        Returns:\n",
    "            dict: The chat history data as a dictionary.\n",
    "        \"\"\"\n",
    "        b64_history = bytes(self.data, \"utf-8\")\n",
    "        decompressed_history = zlib.decompress(\n",
    "            b64decode(b64_history)\n",
    "        )\n",
    "        history = json.loads(decompressed_history.decode())\n",
    "        \n",
    "        if chat_title: \n",
    "            return history.get(chat_title, history)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def set_history(\n",
    "        self,\n",
    "        gemini_chat_history: List[genai.types.StrictContentType], \n",
    "        chat_title:str\n",
    "    ) -> None: \n",
    "        \"\"\"\n",
    "        Set the chat history data for a specific chat title.\n",
    "\n",
    "        Parameters:\n",
    "            gemini_chat_history (List[genai.types.StrictContentType]): The chat history data to be set.\n",
    "            chat_title (str): The title of the chat to set the history.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        history = self.get_history()\n",
    "        history[chat_title] = self.parse_geminichat_history(gemini_chat_history)\n",
    "\n",
    "        bytes_history = bytes(json.dumps(history), \"utf-8\")\n",
    "        compressed_history = zlib.compress(bytes_history)\n",
    "        b64_history = b64encode(compressed_history).decode()\n",
    "        self.data = b64_history\n",
    "\n",
    "    def parse_geminichat_history(\n",
    "        self,\n",
    "        gemini_chat_history: List[genai.types.StrictContentType]\n",
    "    ) -> List[Dict]: \n",
    "        \"\"\"\n",
    "        Parse Gemini chat history data into a list of dictionaries.\n",
    "\n",
    "        Parameters:\n",
    "            gemini_chat_history (List[genai.types.StrictContentType]): The Gemini chat history data to parse.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A list of dictionaries representing the parsed chat history items.\n",
    "        \"\"\"\n",
    "        gemini_chat_history_items = []\n",
    "        \n",
    "        for content in gemini_chat_history: \n",
    "            gemini_chat_history_items.append({\n",
    "                \"role\": content.role,\n",
    "                \"parts\": [\n",
    "                    { \"text\": part.text } for part in content.parts\n",
    "                ]\n",
    "            })\n",
    "        return gemini_chat_history_items\n",
    "\n",
    "    def __str__(self) -> str: \n",
    "        \"\"\"\n",
    "        Returns a string representation of the AiChatHistory object.\n",
    "\n",
    "        Returns:\n",
    "            str: A string representation of the AiChatHistory object.\n",
    "        \"\"\"\n",
    "        return pretty_class(self)\n",
    "\n",
    "class WhatsAppUser: \n",
    "    \"\"\"\n",
    "    Represents a user on WhatsApp with associated chat history.\n",
    "\n",
    "    Attributes:\n",
    "        id (str): The unique identifier of the user.\n",
    "        ai_chat_history (AiChatHistory): The chat history associated with the user.\n",
    "        selected_chat (str or None): The selected chat of the user, or None if not selected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        id:str, \n",
    "        history:str, \n",
    "        selected_chat:str | None\n",
    "    ): \n",
    "        \"\"\"\n",
    "        Initializes a new instance of WhatsAppUser.\n",
    "\n",
    "        Parameters:\n",
    "            id (str): The unique identifier of the user.\n",
    "            history (str): The chat history associated with the user.\n",
    "            selected_chat (str or None): The selected chat of the user, or None if not selected.\n",
    "        \"\"\"\n",
    "\n",
    "        self.id = id\n",
    "        self.ai_chat_history = AiChatHistory(history)\n",
    "        self.selected_chat = selected_chat\n",
    "\n",
    "    def get_ai_response(self, prompt: Optional[str] = \"hi\") -> str: \n",
    "        \"\"\"\n",
    "        Generate a response from an AI model based on the given prompt.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str, optional): The prompt to generate the AI response (default is \"hi\").\n",
    "\n",
    "        Returns:\n",
    "            str: The AI-generated response.\n",
    "        \"\"\" \n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        ai_chat_history = self.ai_chat_history\n",
    "\n",
    "        if self.selected_chat:\n",
    "            history = ai_chat_history.get_history(self.selected_chat)\n",
    "            chat = model.start_chat(history=history)\n",
    "        else:\n",
    "            chat = model.start_chat()\n",
    "            judul = chat.send_message(\"berikan satu judul yang cocok untuk percakapan ini, judulnya saja!\")\n",
    "            self.selected_chat = judul.text\n",
    "            history = ai_chat_history.get_history(self.selected_chat)\n",
    "\n",
    "        response = chat.send_message(prompt)\n",
    "        ai_chat_history.set_history(history, self.selected_chat)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def __str__(self) -> str: \n",
    "        \"\"\"\n",
    "        Returns a string representation of the WhatsAppUser object.\n",
    "\n",
    "        Returns:\n",
    "            str: A string representation of the WhatsAppUser object.\n",
    "        \"\"\"\n",
    "        return pretty_class(self)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "users = get_users(\"./wa_users.json\")\n",
    "\n",
    "for id, item in users.items(): \n",
    "    wa_user = WhatsAppUser(id, **item)\n",
    "    \n",
    "wa_user.get_ai_response(\"apa bumi itu datar?\")\n",
    "print()l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AiChatHistory(\\n  data=\"eJydVMFu2zAM/RXOx8DLB+QyrIcOQy8FVmCHpQe61mJFMm3I4rqg6L/vUfLQoGuHbpdAscjH9x5JPTQXOnq60Mh5R5ccMtOPZUtXkyyzT7x42kuzo28PTZqiw6nRxaWmpWbmlJd6ld3PbFc8M3WG57NSz5nTh+bx9rGlp+xx6l18Jf3G9xxaKoyynSvGlj72wGU54UunIQM/jp4HOrEcaHSictQQWBA03HMF6FzqnGS1FIhraXGzS9nv9rKXDW02104OPKKAoEzyFJUTATDwwrvNhi6nPL3/jp+KV2r1nsfOxz8TCPIGWji76DOuhXjJaZIpv4HfFHlbOV3BwVhLDT7WinTHIfE9x8LqC3OmUMIAfGQdKpmZJbNvKYKgIq+GsBVF3GFFyy5F13nLGTRqS8GNClECv+0+8okTfN7SZ/EvMIeDowbG/6JhdBHYAfVW/p9cGli4WC6FrgMhpsP59xZgJvE3SpF75AwpM/dchlHQrshRn7dxe9Y7QxZQEoQUOCt47RZ4lU1pV/yDD0//zAp0KHo5+HXOWDC0pfprol+Wem3EerPiHlujVvurHchGarCWWNF1GHJRwWOVA419HQA+sB0iEjCFKn51PnBC+EuDUuDwaVZsRvVrUWv5yWZoL2erEhki/3dftnTjprSaVNaQBoMFVaixm/D0RBTcdWN9r+ZSXYjz6qD3/DH4y1MCOt4YYqWUjtrruhZ3090USAtRTCN2Aw0W8oLZL2HgiKQjv/uXl+ctjyDgbh9/ASSI1MY=\"\\n)'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wa_user.ai_chat_history.__str__()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = \"6281818475959\"\n",
    "# user = users[id]\n",
    "\n",
    "# for x in range(25): \n",
    "#     response = get_ai_response(user, \"hi apakabar kamu\")\n",
    "#     time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_response(prompt=\"hi\"): \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    chat = model.start_chat()\n",
    "    response = chat.send_message(prompt)\n",
    "    return chat, response\n",
    "    # judul = chat.send_message(\"berikan satu judul yang cocok untuk percakapan ini, judulnya saja!\")\n",
    "    # user[\"selected_chat\"] = judul.text\n",
    "    # set_history(user, chat.history, judul.text)\n",
    "\n",
    "    # return response\n",
    "\n",
    "chat, response = get_ai_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/gemini-1.5-flash',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "    ),\n",
       "    history=[glm.Content({'parts': [{'text': 'hi'}], 'role': 'user'}), glm.Content({'parts': [{'text': 'Hi there! Ho...you today? \\n'}], 'role': 'model'})]\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_geminichat_history() missing 1 required positional argument: 'gemini_chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/lol/test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [ {} ]\n\u001b[1;32m      <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# gemini_chat_history_items = []\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \n\u001b[1;32m      <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# for content in gemini_chat_history: \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#     })\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# return gemini_chat_history_items\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://8354-35-201-249-3.ngrok-free.app/root/lol/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m parse_geminichat_history()[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_geminichat_history() missing 1 required positional argument: 'gemini_chat_history'"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_geminichat_history(\n",
    "    gemini_chat_history: List[glm.Content]) -> List[Dict]: \n",
    "    return [ {} ]\n",
    "\n",
    "    # gemini_chat_history_items = []\n",
    "    \n",
    "    # for content in gemini_chat_history: \n",
    "    #     gemini_chat_history_items.append({\n",
    "    #         \"role\": content.role,\n",
    "    #         \"parts\": [\n",
    "    #             { \"text\": part.text } for part in content.parts\n",
    "    #         ]\n",
    "    #     })\n",
    "    # return gemini_chat_history_items\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
